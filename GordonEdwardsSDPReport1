\documentclass[12pt]{article}
\usepackage[margin=0.6in]{geometry} 
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{caption}

\sloppy

\begin{document}
\author{s1128614}
\title{IAML Assignment 4}
\maketitle 

\section*{Question 1}
\subsection*{(a)}
All of the clusters expect 2 correspond with classes. Classes 3 is more confused with 2 because the two classes are both quite similar - they are both in the same field - and many words in the vocabulary frequently appear in both.

\subsection*{(b)}
\begin{figure}[ht!]
\centering
\includegraphics[height=50mm]{graph1.png}
\caption{No. of clusters vs Within-cluster Sum of Square Errors}
\label{overflow}
\end{figure}
\noindent Looking at the graph 5 would be the optimal number of clusters because the second derivative at 5 is 0; it is where the steep gradients end and a series of slight gradients being, i.e. the “mountain“ ends and the “rubble” begins. The are five classes which one would naturally expect as there are five classes, so one to represent each class.
\\\\It's not safe to just consider one seed because different clusters can be learnt for different initial cluster centres, so only using the one set of centres each time may not yield the optimal result.
\pagebreak
\subsection*{(c)}
\begin{table}[ht]
\caption{Clusters Assignments for each Class:} 
\vspace{11pt}
\centering 
\begin{tabular}{|p{5cm}|p{4cm}|p{7cm}|} % centered columns (4 columns)
\hline                     
Class & Assigned Cluster & Reason \\ [0.5ex] % inserts table 
%heading
\hline                 
1:alt.atheism & 5 & The word with the most dominant importance weight of the cluster concerns the alt.atheism class, and the words "atheism" and "religion" have the high importance weights.\\ \hline
2:comp.sys.ibm.pc.hardware & 2 & Most of the words with the highest importance weights relate to the comp.sys.ibm.pc.hardware class.\\ \hline
3:comp.sys.mac.hardware & 1 & The words "apple" and "mac" appear within the set of highest importance weights.\\ \hline
4:rec.sport.baseball & 4 & The second highest importance weight is "baseball" followed by other words related to the rec.sport.baseball class. \\\hline
5:rec.sport.hockey & 3 & The highest importance weight is hockey, followed by other words related to the rec.sport.hockey  class.\\ \hline %inserts single line
\end{tabular}
\end{table}

\vspace{11pt} \noindent The 3rd and 4th clusters are closest because there are many words in set of highest importance weights of each cluster which are common to both clusters.
\section*{Question 2}
\subsection*{(a)}
Percent Correct:
\\Naive Bayes: 71.15
\\SVM:  88.5 
\\\\All the pixels whose intensity is zero (hence the variance in intensity is also zero) and whose distribution between the digit classes is even are redundant as they provide nothing that can be used in distinguishing the digit class of an image. Using \texttt{RemoveUseless} in WEKA removes 145 attributes, those whose variance is too large or too small; in our case the redundant attributes with low variance will be removed. The percentage correct for both classifiers is the same as before removing the 145 attributes, confirming their redundancy.
\pagebreak
\subsection*{(b)}
\begin{figure}[ht!]
\centering
\includegraphics[height=70mm]{graph2.png}
\caption{Rank vs Eigenvalues}
\label{overflow}
\end{figure}
\noindent \\The graph shows that there are only a small proportion of the attributes, less than a fifth, are worth choosing, those with corresponding eigenvalues which maximise the variance, as most of the attributes have corresponding eigenvectors with give low variance. So it is possible to remove more than 80\% of the attributes, and just capture those necessary ones which retain 100\% of the variance.\\\\
\subsection*{(c)}
\begin{minipage}{.5\textwidth}
%\centering
\textbf{Naive Bayes:}
\\Percent Correct: 82.8
\\MAE: 0.3841
\\RMSE: 0.4533
\end{minipage}%
\begin{minipage}{.5\textwidth}
%\centering
\textbf{SVM:}
\\Percent Correct: 88.75
\\MAE: 0.0351
\\RMSE: 0.1707
\end{minipage}
\\\\\\
The performance increases in Naive Bayes a lot, but SVM stays the same. Using PCA in WEKA on the data has the effect of turning the distributions into Gaussians, so a similar effect to normalising the data, meaning that \texttt{NaiveBayes} performs better. The SVM kernel has the effect of transforming the attributes along a dimension like PCA, meaning performing PCA before using the SVM doesn't have much effect because the data is already transformed.
\section*{Question 3}
Percent Correct with exponent 1.0 on 1st data set: 90.2
\\Percent Correct with exponent 2.0 on 2nd data set: 89.55

\end{document}